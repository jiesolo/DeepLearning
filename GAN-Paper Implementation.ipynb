{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7357aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84f34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbbba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The config class defines the hyperparameter,\n",
    "class Config(object):\n",
    "    data_path = \"D:/Project/demo1/NN-Torch/aml-data\"\n",
    "    virs = \"result\"\n",
    "    num_workers = 0  # multithreading, An error occured on windows operating system\n",
    "    img_size = 96  # cut the pixel size of the image\n",
    "    batch_size = 256  # number of batches\n",
    "    max_epoch = 400   # the maximum epoch\n",
    "    lr1 = 2e-4  # Generator learning rate\n",
    "    lr2 = 2e-4  # Discriminator learning rate\n",
    "    beta1 = 0.5  # Regularization coefficient, Adam optimizer parameter\n",
    "    gpu = True  # Using GPU computing\n",
    "    nz = 100  # Noise dimension\n",
    "    ngf = 64  # Number of convolution kernels for generator\n",
    "    ndf = 64  # The number of convolution kernels of the discriminator\n",
    "\n",
    "    # 1.Model saving path\n",
    "    save_path = 'D:/Project/demo1/NN-Torch/imgs2/'  # opt.netg path Save path of the generated image\n",
    "    # The update frequency of the discriminative model is higher than that of the generative model\n",
    "    d_every = 1  # Each batch trains the discriminator once\n",
    "    g_every = 5  # The generation model is trained every batch\n",
    "    save_every = 5  # each save_every save the model\n",
    "    netd_path = None\n",
    "    netg_path = None\n",
    "\n",
    "    # testing data\n",
    "    gen_img = \"result.png\"\n",
    "    # choose the save pic\n",
    "    # Save 64 images at a time\n",
    "    gen_num = 64\n",
    "    gen_search_num = 512\n",
    "    gen_mean = 0    # Generate the noise mean of the model\n",
    "    gen_std = 1     # Noise variance\n",
    "\n",
    "# Instantiate the Config class, set the hyperparameter, and set it as a global parameter\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc9372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e08f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generation model to generate images by input noise vector\n",
    "class NetG(nn.Module):\n",
    "    # Build the initialization function and pass in the opt class\n",
    "    def __init__(self, opt):\n",
    "        super(NetG, self).__init__()\n",
    "        # Number of feature maps of self.ngf generator\n",
    "        self.ngf = opt.ngf\n",
    "        self.Gene = nn.Sequential(\n",
    "            # Suppose the input is opt.nz*1*1 dimension data, opt.nz dimension vector\n",
    "            # output = (input - 1)*stride + output_padding - 2*padding + kernel_size\n",
    "            # What does it mean to expand the convolution of a pixel and let the machine learn to understand each element of the n\n",
    "            nn.ConvTranspose2d(in_channels=opt.nz, out_channels=self.ngf * 8, kernel_size=4, stride=1, padding=0, bias =False),\n",
    "            nn.BatchNorm2d(self.ngf * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # input 4*4*ngf*8\n",
    "            nn.ConvTranspose2d(in_channels=self.ngf * 8, out_channels=self.ngf * 4, kernel_size=4, stride=2, padding=1, bias =False),\n",
    "            nn.BatchNorm2d(self.ngf * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # input 8*8*ngf*4\n",
    "            nn.ConvTranspose2d(in_channels=self.ngf * 4, out_channels=self.ngf * 2, kernel_size=4, stride=2, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # input 16*16*ngf*2\n",
    "            nn.ConvTranspose2d(in_channels=self.ngf * 2, out_channels=self.ngf, kernel_size=4, stride=2, padding=1, bias =False),\n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # input 32*32*ngf\n",
    "            nn.ConvTranspose2d(in_channels=self.ngf, out_channels=3, kernel_size=5, stride=3, padding=1, bias =False),\n",
    "\n",
    "            # The convergence speed of Tanh is faster than sigmoid and much slower than relu. The output range is [-1,1], and the output mean is 0\n",
    "            nn.Tanh(),\n",
    "\n",
    "        )# 输出一张96*96*3\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Gene(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92885ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Discriminator\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(NetD, self).__init__()\n",
    "\n",
    "        self.ndf = opt.ndf\n",
    "        # Discriminator defined by DCGAN, generator has no pooling layer\n",
    "        self.Discrim = nn.Sequential(\n",
    "            # Convolution layer\n",
    "            # The number of input channels in_channels, the number of output channels (that is, the number of channels of the convolution kernel)out_channels. Here, \n",
    "            # 64 filer filters are set, and the output channel is 64 naturally.\n",
    "            # Because the image is gray processed, the number of channels here is 1,\n",
    "            # input:(bitch_size, 3, 96, 96),bitch_size = Sample size for a single training session\n",
    "            # output:(bitch_size, ndf, 32, 32), (96 - 5 +2 *1)/3 + 1 =32\n",
    "            # LeakyReLu= x if x>0 else nx (n is the parameter of the first function)，\n",
    "            # Enabling inplace (overwrite) saves memory and eliminates the process of repeatedly requesting memory\n",
    "            # LeakyReLu cancels the negative hard saturation problem of Relu, and whether it is effective for model optimization remains to be verified\n",
    "            nn.Conv2d(in_channels=3, out_channels= self.ndf, kernel_size= 5, stride= 3, padding= 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace= True),\n",
    "\n",
    "            # input:(ndf, 32, 32)\n",
    "            nn.Conv2d(in_channels= self.ndf, out_channels= self.ndf * 2, kernel_size= 4, stride= 2, padding= 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            # input:(ndf *2, 16, 16)\n",
    "            nn.Conv2d(in_channels= self.ndf * 2, out_channels= self.ndf *4, kernel_size= 4, stride= 2, padding= 1,bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            # input:(ndf *4, 8, 8)\n",
    "            nn.Conv2d(in_channels= self.ndf *4, out_channels= self.ndf *8, kernel_size= 4, stride= 2, padding= 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf *8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            # input:(ndf *8, 4, 4)\n",
    "            # output:(1, 1, 1)\n",
    "            nn.Conv2d(in_channels= self.ndf *8, out_channels= 1, kernel_size= 4, stride= 1, padding= 0, bias=True),\n",
    "\n",
    "            # Call the sigmoid function to solve the classification problem\n",
    "            # Because the discriminant model needs to do dichotomous classification, it is sufficient to use sigmoid, \n",
    "            # because the return value interval of sigmoid is [0,1],\n",
    "            # It can be used as a scoring standard for the discriminative model\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten and return\n",
    "        return self.Discrim(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4647c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4bdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "\n",
    "    # Configure properties\n",
    "    # Use the default hyperargument set in opt if the function has no dictionary input\n",
    "    for k_, v_ in kwargs.items():\n",
    "        setattr(opt, k_, v_)\n",
    "\n",
    "    # Use gpu or cpy\n",
    "    if opt.gpu:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Data preprocessing\n",
    "    # transforms Module provides general image conversion operation class functions, and finally converted to floatTensor\n",
    "    # tv.transforms.Compose used for tv.transforms operation,Once defined, \n",
    "    # the transforms combination is passed directly into the image for processing\n",
    "    # tv.transforms.Resize，resize the PIL Image object and save the value in float64\n",
    "    # tv.transforms.CenterCrop, Center clipping\n",
    "    # tv.transforms.ToTensor，torch image type (channel, pixel, pixel) and change the pixel range to [0,1]\n",
    "    # tv.transforms.Normalize,Perform image = (image-mean)/std data normalization operation, \n",
    "    # with mean as one parameter and std as two parameters\n",
    "    # Because it is a three-channel, mean = (0.5, 0.5, 0.5), which is converted into the range of [-1, 1]\n",
    "    transforms = tv.transforms.Compose([\n",
    "        # 3*96*96\n",
    "        tv.transforms.Resize(opt.img_size),   # scale it to img_size* img_size\n",
    "        # The center is cropped into a 96 by 96 image. Because the data of this experiment has met the 96*96 size, it can be omitted\n",
    "        tv.transforms.CenterCrop(opt.img_size),\n",
    "\n",
    "        # ToTensor and Normalize collocation\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load the data and preprocess the image with defined transforms, which was defined directly\n",
    "    dataset = tv.datasets.ImageFolder(root=opt.data_path, transform=transforms)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,      \n",
    "        batch_size=opt.batch_size,    \n",
    "        shuffle=True,     \n",
    "        #num_workers=opt.num_workers,    \n",
    "        drop_last=True          \n",
    "    )\n",
    "\n",
    "    \n",
    "    netg, netd = NetG(opt), NetD(opt)\n",
    "    # Determine whether the network has a weight value\n",
    "    # storage\n",
    "    map_location = lambda storage, loc: storage\n",
    "\n",
    "\n",
    "\n",
    "    if opt.netg_path:\n",
    "        netg.load_state_dict(torch.load(f=opt.netg_path, map_location=map_location))\n",
    "    if opt.netd_path:\n",
    "        netd.load_state_dict(torch.load(f=opt.netd_path, map_location=map_location))\n",
    "\n",
    "    netd.to(device)\n",
    "    netg.to(device)\n",
    "\n",
    "    optimize_g = torch.optim.Adam(netg.parameters(), lr=opt.lr1, betas=(opt.beta1, 0.999))\n",
    "    optimize_d = torch.optim.Adam(netd.parameters(), lr=opt.lr2, betas=(opt.beta1, 0.999))\n",
    "\n",
    "\n",
    "    criterions = nn.BCELoss().to(device)\n",
    "\n",
    "\n",
    "    true_labels = torch.ones(opt.batch_size).to(device)\n",
    "    fake_labels = torch.zeros(opt.batch_size).to(device)\n",
    "\n",
    "\n",
    "    noises = torch.randn(opt.batch_size, opt.nz, 1, 1).to(device)\n",
    "\n",
    "\n",
    "    fix_noises = torch.randn(opt.batch_size, opt.nz, 1, 1).to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        for ii_, (img, _) in tqdm((enumerate(dataloader))):\n",
    "            real_img = img.to(device)\n",
    "\n",
    "\n",
    "            if ii_ % opt.d_every == 0:\n",
    "                optimize_d.zero_grad()\n",
    "                output = netd(real_img)\n",
    "                error_d_real = criterions(output, true_labels)\n",
    "                error_d_real.backward()\n",
    "                noises = noises.detach()\n",
    "                fake_image = netg(noises).detach()\n",
    "                output = netd(fake_image)\n",
    "                error_d_fake = criterions(output, fake_labels)\n",
    "                error_d_fake.backward()\n",
    "                optimize_d.step()\n",
    "\n",
    "            if ii_ % opt.g_every == 0:\n",
    "                optimize_g.zero_grad()\n",
    "                noises.data.copy_(torch.randn(opt.batch_size, opt.nz, 1, 1))\n",
    "                fake_image = netg(noises)\n",
    "                output = netd(fake_image)\n",
    "                error_g = criterions(output, true_labels)\n",
    "                error_g.backward()\n",
    "                optimize_g.step()\n",
    "\n",
    "        if (epoch + 1) % opt.save_every == 0:\n",
    "            fix_fake_image = netg(fix_noises)\n",
    "            tv.utils.save_image(fix_fake_image.data[:64], \"%s/%s.png\" % (opt.save_path, epoch), normalize=True)\n",
    "\n",
    "            torch.save(netd.state_dict(),  'D:/Project/demo1/NN-Torch/imgs2/' + 'netd_{0}.pth'.format(epoch))\n",
    "            torch.save(netg.state_dict(),  'D:/Project/demo1/NN-Torch/imgs2/' + 'netg_{0}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3347a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(**kwargs):\n",
    "    for k_, v_ in kwargs.items():\n",
    "        setattr(opt, k_, v_)\n",
    "\n",
    "    device = torch.device(\"cuda\") if opt.gpu else torch.device(\"cpu\")\n",
    "    netg, netd = NetG(opt).eval(), NetD(opt).eval()\n",
    "    map_location = lambda storage, loc: storage\n",
    "    netd.load_state_dict(torch.load('D:/Project/demo1/NN-Torch/imgs2/netd_399.pth', map_location=map_location), False)\n",
    "    netg.load_state_dict(torch.load('D:/Project/demo1/NN-Torch/imgs2/netg_399.pth', map_location=map_location), False)\n",
    "    netd.to(device)\n",
    "    netg.to(device)\n",
    "    noise = torch.randn(opt.gen_search_num, opt.nz, 1, 1).normal_(opt.gen_mean, opt.gen_std).to(device)\n",
    "\n",
    "    fake_image = netg(noise)\n",
    "    score = netd(fake_image).detach()\n",
    "    indexs = score.topk(opt.gen_num)[1]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for ii in indexs:\n",
    "        result.append(fake_image.data[ii])\n",
    "    tv.utils.save_image(torch.stack(result), opt.gen_img, normalize=True, value_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6190b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train()\n",
    "    generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199cd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [03:26,  3.22s/it]\n",
      "64it [00:22,  2.83it/s]\n",
      "64it [00:22,  2.80it/s]\n",
      "64it [00:22,  2.80it/s]\n",
      "64it [00:23,  2.77it/s]\n",
      "64it [00:23,  2.77it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.63it/s]\n",
      "64it [00:24,  2.59it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:24,  2.65it/s]\n",
      "64it [00:24,  2.65it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.65it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.64it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:24,  2.57it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.64it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:24,  2.61it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.63it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.76it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.76it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:24,  2.65it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.75it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.74it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:24,  2.64it/s]\n",
      "64it [00:24,  2.62it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.72it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.67it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.71it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:24,  2.64it/s]\n",
      "64it [00:24,  2.63it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:24,  2.67it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:23,  2.67it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:24,  2.66it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.68it/s]\n",
      "64it [00:24,  2.60it/s]\n",
      "64it [00:23,  2.69it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.73it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n",
      "64it [00:23,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3e890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_data_dir = r'D:/Project/demo1/NN-Torch/aml-data'\n",
    "batchsize = 32\n",
    "train_data_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(128),  # 128\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(train_data_dir, transform=train_data_transforms)\n",
    "\n",
    "train_data_loader = Data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "# 构建鉴别器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 初始化父类\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Conv2d(256, 3, kernel_size=8, stride=2),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3 * 10 * 10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.feature(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 构建生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(100, 3 * 11 * 11),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 3, kernel_size=8, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(batchsize, 3, 11, 11)\n",
    "        x = self.feature(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "d_optimizer = torch.optim.SGD(D.parameters(), lr=0.01)\n",
    "g_optimizer = torch.optim.SGD(G.parameters(), lr=0.01)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d_z_loss = []\n",
    "    epoches = 25\n",
    "    for epoch in range(epoches):\n",
    "        print('开始第', epoch + 1, '轮', '*******************' * 3)\n",
    "        sum_d_loss = 0\n",
    "        for step, (b_x, _) in enumerate(train_data_loader):  # b_x shape(32,3,128,128)\n",
    "            # 训练辨别器,real data\n",
    "            real_out = D(b_x.to(device))\n",
    "            real_out = real_out.squeeze()  # (batch_size,1) -> (batchsize)\n",
    "            real_label = torch.ones(batchsize).to(device)\n",
    "            d_loss_real = loss_func(real_out, real_label)  # 标签为1,为真实样本,辨别器的loss\n",
    "            # fake data\n",
    "            fake_img = G(torch.rand(batchsize, 100).to(device)).detach()\n",
    "            fake_out = D(fake_img).squeeze()\n",
    "            fake_label = torch.zeros(batchsize).to(device)\n",
    "            d_loss_fake = loss_func(fake_out, fake_label)\n",
    "            # 更新辨别器\n",
    "            d_loss = (d_loss_real + d_loss_fake)\n",
    "            sum_d_loss += d_loss.item()\n",
    "            d_optimizer.zero_grad()  # 在反向传播之前，先将梯度归0\n",
    "            d_loss.backward()  # 将误差反向传播\n",
    "            d_optimizer.step()  # 更新参数\n",
    "\n",
    "            # 训练生成器\n",
    "            fake_img = G(torch.rand(batchsize, 100).to(device))\n",
    "            output = D(fake_img).squeeze()\n",
    "            g_loss = loss_func(output, real_label)\n",
    "\n",
    "            g_optimizer.zero_grad()  # 梯度归0\n",
    "            g_loss.backward()  # 进行反向传播\n",
    "            g_optimizer.step()  # .step()一般用在反向传播后面,用于更新生成网络的参数\n",
    "\n",
    "            fake_img = G(torch.rand(batchsize, 100).to(device))\n",
    "            output = D(fake_img).squeeze()\n",
    "            g_loss = loss_func(output, real_label)\n",
    "\n",
    "            g_optimizer.zero_grad()  # 梯度归0\n",
    "            g_loss.backward()  # 进行反向传播\n",
    "            g_optimizer.step()  # .step()一般用在反向传播后面,用于更新生成网络的参数\n",
    "\n",
    "            print('{:.5%}'.format(step / len(train_data_loader)))\n",
    "\n",
    "        d_z_loss.append(sum_d_loss / len(train_data_loader))\n",
    "        print(d_z_loss)\n",
    "        if epoch % 2 == 0:\n",
    "            name = 'face' + str(epoch) + '.pth'\n",
    "            torch.save(G, name, _use_new_zipfile_serialization=False)\n",
    "\n",
    "    torch.save(D, 'Generator.pth', _use_new_zipfile_serialization=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
